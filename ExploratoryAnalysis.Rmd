---
title: "Exploratory Analysis of Text Prediction Data"
author: "Charlie LeCrone"
date: "December 31, 2017"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 8
    keep_md: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

This document will detail the exploratory analysis I've done with the text data I am using to build a predictive text model.  I will provide some basic statistics on the large data set provided.  Then I will provide some estimated frequencies and distributions of the words / tokens and phrases / n-grams projected from a random sample of the larger data set.  Finally, I'll provide a summary of my take on the data and how I plan to proceed in developing a text prediction algorithm. 

```{r, echo=FALSE}

load(file="counts.Rda")
load(file="corpdocs.Rda")
load(file="corpfeatures.Rda")
load(file="corpdocs.Rda")
load(file="corpfeatures.Rda")
options(scipen=1, digits = 2)
totalwords<-counts$blogwords+counts$newswords+counts$tweetwords

```

## Data Overview

The data set includes three large text files.

The en_us.blogs.txt file is a collection of text from blogs.  It includes:

- `r formatC(counts$blogdocs, big.mark=",")` documents
- `r formatC(counts$blogwords, big.mark=",")` words
- `r formatC(counts$blogwordsperdoc, big.mark=",")` words per document

The en_us.news.txt file is a collection of text from news outlets.  It includes:

- `r formatC(counts$newsdocs, big.mark=",")` documents
- `r formatC(counts$newswords, big.mark=",")` words
- `r formatC(counts$newswordsperdoc, big.mark=",")` words per document

The en_us.tweets file is a collection of text from tweets.  It includes:

- `r formatC(counts$tweetdocs, big.mark=",")` documents
- `r formatC(counts$tweetwords, big.mark=",")` words
- `r formatC(counts$tweetwordsperdoc, big.mark=",")` words per document

I read all three of these files into a single data frame, then used the quanteda package to create a single large corpus of all the text data.  The large corpus contains:

- `r formatC(corpdocs, big.mark=",")` documents
- `r formatC(corpfeatures, big.mark=",")` unique words

`r formatC(corpfeatures, big.mark=",")` unique words comprise a total language of `r formatC(totalwords, big.mark=",")` words.  This indicates that there are a small set of words that are used frequently.  This information should be useful as I begin to develop a model.

## Frequency Analysis

``` {r, echo=F}

load(file="tbl1.Rda")
load(file="tbl2.Rda")
load(file="tbl3.Rda")

# define intercepts where we reach 50 and 90 percent of total n-grams

y1a<-tbl1 %>%
  filter(pcttoken>=.5)
yint1a<-min(y1a$pctdoc)

y1b<-tbl1 %>%
  filter(pcttoken>=.9)
yint1b<-min(y1b$pctdoc)

y2a<-tbl2 %>%
  filter(pcttoken>=.5)
yint2a<-min(y2a$pctdoc)

y2b<-tbl2 %>%
  filter(pcttoken>=.9)
yint2b<-min(y2b$pctdoc)

y3a<-tbl3 %>%
  filter(pcttoken>=.5)
yint3a<-min(y3a$pctdoc)

y3b<-tbl3 %>%
  filter(pcttoken>=.9)
yint3b<-min(y3b$pctdoc)
```

To conserve system resources, used the quanteda corpus_sample() function to randomly sample 5% of the larger corpus.  With this sample I analyzed the frequency distribution of 1, 2, and 3 grams to get an idea of how many n-grams I would need to cover the majority of the language in the corpus.

### Unigrams

The plot below shows the top 20 unigrams:

![](Columns1.png)

This next plot shows what percentage of unique unigrams I need to cover 50 and 90 percent of the total unigrams in the sample.  We can see that we only need a small 

